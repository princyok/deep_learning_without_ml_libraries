{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Prince Okoli — [GitHub](https://github.com/princyok/deep_learning_without_ml_libraries) — [Blog series](https://princyok.github.io/demonstration-of-the-models-in-action.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration using the Breast Cancer Wisconsin data\n",
    "\n",
    "The Breast cancer Wisconsin dataset is a dataset that describe the characteristics of the cell nuclei for 569 breast lumps, and includes whether they are malignant or benign. Published by Clinical Science Center, University of Wisconsin.\n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n",
    "\n",
    "**This Jupyter notebook will put to the test the artificial neuron that was written from scratch without dependence on any ML libraries.** Machine learning libraries may be used in this notebook to make data preprocessing cleaner and more efficient, but not for the actual learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import artificial_neuron as an\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import model_selection, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"../datasets/breast-cancer-wisconsin-data/data.csv\", delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.head())\n",
    "display(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the target (\"daignosis\" column) from nominal strings (\"M\" and \"B\") to numbers (0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"diagnosis\"]=preprocessing.LabelEncoder().fit([\"M\",\"B\"]).transform(data[\"diagnosis\"]) # M is 0, B is 1.\n",
    "\n",
    "data.drop(columns=\"id\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the pandas Dataframe to numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test sets, and also seperate into features (x) and target (y). Also ensure the shape of the input and output tensors are as expected. For features: num_features x batch_size. For target: 1 x batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = model_selection.train_test_split(data, train_size=0.8, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=data_train.T\n",
    "data_test=data_test.T\n",
    "\n",
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=data_train[1:, :]\n",
    "y_train=data_train[0, :].reshape(1,-1)\n",
    "\n",
    "x_test=data_test[1:, :]\n",
    "y_test=data_test[0, :].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just start with some randomly chosen hyperparameters:\n",
    "\n",
    "* number of iterations: 10\n",
    "\n",
    "* learning rate (step size): 0.00007\n",
    "\n",
    "* batch size: 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron=an.Neuron(X=x_train, Y=y_train)\n",
    "neuron.train(num_iterations=10, learning_rate=7e-5, batch_size=256)\n",
    "\n",
    "print(\"training accuracy: \", neuron.evaluate(X=x_train, Y=y_train, metric=\"accuracy\"))\n",
    "print(\"test accuracy:     \", neuron.evaluate(X=x_test, Y=y_test, metric=\"accuracy\"), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neuron.a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We likely ran into an overflow or underflow somewhere which led to the NaNs. We can try to avoid this by tweaking our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaious_iterations=[50, 100, 500, 1000]\n",
    "for n in vaious_iterations:\n",
    "    neuron=an.Neuron(X=x_train, Y=y_train)\n",
    "    neuron.train(num_iterations=n, learning_rate=7e-6, batch_size=32)\n",
    "\n",
    "    print(\"training accuracy: \", neuron.evaluate(X=x_train, Y=y_train, metric=\"accuracy\"))\n",
    "    print(\"test accuracy:     \", neuron.evaluate(X=x_test, Y=y_test, metric=\"accuracy\"), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start seeing the training accuracy outpace the test accuracy by the time we are reaching 1000 iterations, so we stop there as that is possibly a sign that we are begining to overfit (a little overfitting is perfectly fine).\n",
    "\n",
    "**We have 91% test accuracy after 500 iterations.**\n",
    "\n",
    "Let's play around with the other hyperparameters to see if we can get something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learning_rates=[7e-7,7e-6,7e-5,7e-4]\n",
    "for lr in learning_rates:    \n",
    "    neuron=an.Neuron(X=x_train, Y=y_train)\n",
    "    neuron.train(num_iterations=700, learning_rate=lr, batch_size=32)\n",
    "\n",
    "    print(\"training accuracy: \", neuron.evaluate(X=x_train, Y=y_train, metric=\"accuracy\"))\n",
    "    print(\"test accuracy:     \", neuron.evaluate(X=x_test, Y=y_test, metric=\"accuracy\"), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We overflowed/underflowed when our learning rate got relatively too high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out different batch sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate the printout for np array.\n",
    "np.set_printoptions(threshold=6)\n",
    "\n",
    "batch_sizes=[8, 32, 64, 256]\n",
    "for b in batch_sizes:\n",
    "    neuron=an.Neuron(X=x_train, Y=y_train)\n",
    "    neuron.train(num_iterations=500, learning_rate=7e-6, batch_size=b)\n",
    "    \n",
    "    print(\"training accuracy: \", neuron.evaluate(X=x_train, Y=y_train, metric=\"accuracy\"))\n",
    "    print(\"test accuracy:     \", neuron.evaluate(X=x_test, Y=y_test, metric=\"accuracy\"))\n",
    "    print(neuron.a, end=\"\\n\\n\")\n",
    "\n",
    "# resetting to the default.\n",
    "np.set_printoptions(threshold=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this learning are, we are getting good performance with less batch size, but this doesn't mean we will always do better with smaller batch sizes.\n",
    "\n",
    "Let's shoot for the highest test accuracy possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron=an.Neuron(X=x_train, Y=y_train)\n",
    "neuron.train(num_iterations=1300, learning_rate=9e-6, batch_size=256)\n",
    "\n",
    "print(\"training accuracy: \", neuron.evaluate(X=x_train, Y=y_train, metric=\"accuracy\"))\n",
    "print(\"test accuracy:     \", neuron.evaluate(X=x_test, Y=y_test, metric=\"accuracy\"), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We achieved 92%**. Maybe we can get one or two more % if we keep trying."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
